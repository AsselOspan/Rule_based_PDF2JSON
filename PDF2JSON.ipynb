{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed7db176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz   # PyMuPDF\n",
    "import re\n",
    "import json\n",
    "\n",
    "# PDF filename (newspaper issue)\n",
    "pdf_path = \"01.01.2021.pdf\"\n",
    "\n",
    "# Open the PDF with PyMuPDF\n",
    "doc = fitz.open(pdf_path)\n",
    "\n",
    "# Prepare mapping for CID placeholders to actual Kazakh characters\n",
    "# (This mapping may need adjustment per PDF; here are common ones)\n",
    "cid_to_char = {\n",
    "    'cid:19': 'Ө',  # likely mappings from analysis\n",
    "    'cid:25': 'ө',\n",
    "    'cid:4':  'Ә',\n",
    "    'cid:8':  'ә',\n",
    "    'cid:5':  'і',\n",
    "    # ... (include other mappings as identified)\n",
    "}\n",
    "\n",
    "# Variables to hold global info (journal name, date, number)\n",
    "journal_name = \"\"\n",
    "issue_date = \"\"\n",
    "issue_number = \"\"\n",
    "\n",
    "# Utility function to clean and fix extracted text\n",
    "def fix_text(text):\n",
    "    \"\"\"Replace CID placeholders and Latin stand-ins with correct Kazakh characters.\"\"\"\n",
    "    # Replace (cid:xx) patterns using the mapping\n",
    "    for cid, char in cid_to_char.items():\n",
    "        text = text.replace(f\"({cid})\", char)  # PDFMiner style \"(cid:##)\"\n",
    "        text = text.replace(f\"{cid})\", char)   # sometimes without opening parenthesis\n",
    "        text = text.replace(f\"{cid} \", char)   # or followed by space\n",
    "    # Replace Latin 'ə' with Cyrillic 'ә' (common mis-mapping for Kazakh 'ә')\n",
    "    text = text.replace(\"ə\", \"ә\")\n",
    "    # (Add other replacements if needed, e.g. unusual quotes or hyphen issues)\n",
    "    return text\n",
    "\n",
    "# Function to split text into sentences and return the first N sentences as abstract\n",
    "def get_abstract(text, num_sentences=10):\n",
    "    # Simple sentence splitting by punctuation (. ! ?)\n",
    "    sentences = re.split(r'(?<=[\\.!?])\\s+', text)\n",
    "    abstract = \" \".join(sentences[:num_sentences])\n",
    "    return abstract\n",
    "\n",
    "articles = []  # list to collect article dictionaries\n",
    "article_count = 0\n",
    "\n",
    "# Iterate through pages to extract content\n",
    "for page_index in range(len(doc)):\n",
    "    page = doc.load_page(page_index)\n",
    "    blocks = page.get_text(\"dict\")[\"blocks\"]  # get text blocks with details\n",
    "    \n",
    "    # Sort blocks top-to-bottom, left-to-right for reading order\n",
    "    blocks.sort(key=lambda b: (round(b[\"lines\"][0][\"bbox\"][1]), b[\"lines\"][0][\"bbox\"][0]) if b[\"type\"]==0 else (float('inf'), float('inf')))\n",
    "    \n",
    "    for block in blocks:\n",
    "        if block[\"type\"] != 0:  # skip non-text blocks (images, etc.)\n",
    "            continue\n",
    "        # Concatenate all span texts in this block\n",
    "        block_text = \"\".join(span[\"text\"] for line in block[\"lines\"] for span in line[\"spans\"])\n",
    "        block_text = block_text.strip()\n",
    "        if not block_text:\n",
    "            continue\n",
    "\n",
    "        # Check for header info on first page\n",
    "        if page_index == 0 and not journal_name:\n",
    "            # Look for journal name (assuming it might appear in text if not an image)\n",
    "            # If the newspaper name is not text (logo image), this may remain empty.\n",
    "            match_journal = re.search(r\"Егемен\\s*Қазақстан|Egemen\\s*Qazaqstan\", block_text)\n",
    "            if match_journal:\n",
    "                journal_name = match_journal.group(0)\n",
    "            # Find issue number (e.g. \"№84\")\n",
    "            match_number = re.search(r\"№\\s*\\d+\", block_text)\n",
    "            if match_number:\n",
    "                issue_number = match_number.group(0)\n",
    "            # Find date (e.g. \"1 МАМЫР 2020\" or \"1 Мамыр 2020\")\n",
    "            match_date = re.search(r\"\\d{1,2}\\s*[^0-9\\s]{3,}\\s*\\d{4}\", block_text)\n",
    "            if match_date:\n",
    "                issue_date = match_date.group(0)\n",
    "        \n",
    "        # Use font sizes to detect titles within the block\n",
    "        # Find the maximum font size in this block\n",
    "        max_size = 0\n",
    "        for line in block[\"lines\"]:\n",
    "            for span in line[\"spans\"]:\n",
    "                if span[\"size\"] > max_size:\n",
    "                    max_size = span[\"size\"]\n",
    "        # Determine a threshold: consider text as title if its size is near the max_size of the whole page\n",
    "        # (We could also compute an overall body text size and pick anything significantly larger.)\n",
    "        # Here, we'll use a simple heuristic: titles are likely above 15 pt.\n",
    "        is_title_block = max_size > 15  # can adjust threshold based on typical font sizes\n",
    "        \n",
    "        # If this block contains a title (large text), start a new article\n",
    "        if is_title_block:\n",
    "            # Finalize the previous article (if any is open) – here we assume each title block is a new article.\n",
    "            # (If an article spans multiple blocks, we'll handle continuation below.)\n",
    "            # For simplicity, we treat each title block as a separate article start.\n",
    "            article_count += 1\n",
    "            \n",
    "            # Separate title lines and other lines in the block\n",
    "            title_lines = []\n",
    "            author_line = \"\"\n",
    "            topic_line = \"\"\n",
    "            body_lines = []\n",
    "            \n",
    "            for line in block[\"lines\"]:\n",
    "                # Check each line's first span size\n",
    "                if line[\"spans\"]:\n",
    "                    span0 = line[\"spans\"][0]\n",
    "                    text_line = \"\".join(span[\"text\"] for span in line[\"spans\"]).strip()\n",
    "                    if span0[\"size\"] == max_size:\n",
    "                        title_lines.append(text_line)\n",
    "                        continue\n",
    "                    # Check if line is all caps (topic)\n",
    "                    if text_line.isupper():\n",
    "                        topic_line = text_line\n",
    "                        continue\n",
    "                    # Check if the line is likely an author (italic or different font)\n",
    "                    if \"Italic\" in span0[\"font\"].split('+')[-1] or span0.get(\"flags\", 0) & 2:  # flag 2 might indicate italic in PyMuPDF\n",
    "                        author_line = text_line\n",
    "                        continue\n",
    "                    # Otherwise, it's part of the body text\n",
    "                    body_lines.append(text_line)\n",
    "            \n",
    "            title = \" \".join(title_lines)\n",
    "            title = fix_text(title)\n",
    "            topic = fix_text(topic_line) if topic_line else \"\"\n",
    "            author = fix_text(author_line) if author_line else \"\"\n",
    "            # The body_lines from this block (if any) will be the start of the article text\n",
    "            text_content = \" \".join(body_lines)\n",
    "            \n",
    "            # Continue to next blocks to gather rest of the article until next title encountered\n",
    "            # We peek ahead in the blocks list for consecutive text that likely belongs to this article.\n",
    "            # Stop when we reach another title block or a big gap.\n",
    "            # (In this simple approach, we assume the very next block(s) are continuation if not title blocks.)\n",
    "            continue_index = blocks.index(block) + 1\n",
    "            while continue_index < len(blocks):\n",
    "                next_block = blocks[continue_index]\n",
    "                continue_index += 1\n",
    "                if next_block[\"type\"] != 0:\n",
    "                    continue\n",
    "                # Determine if the next block starts with a large font (another title)\n",
    "                next_max = 0\n",
    "                for line in next_block[\"lines\"]:\n",
    "                    for span in line[\"spans\"]:\n",
    "                        if span[\"size\"] > next_max:\n",
    "                            next_max = span[\"size\"]\n",
    "                if next_max > 15:  # if next block has a large text, assume it's a new article\n",
    "                    break\n",
    "                # Otherwise, treat it as continuation of current article\n",
    "                next_text = \"\".join(span[\"text\"] for line in next_block[\"lines\"] for span in line[\"spans\"]).strip()\n",
    "                text_content += \" \" + next_text\n",
    "            \n",
    "            # Fix text encoding issues in the assembled content\n",
    "            text_content = fix_text(text_content)\n",
    "            \n",
    "            # Abstract: first 10 sentences of text\n",
    "            abstract = get_abstract(text_content, 10)\n",
    "            \n",
    "            # Create article dict\n",
    "            article_data = {\n",
    "                \"title\": title,\n",
    "                \"topic\": topic,\n",
    "                \"author\": author,\n",
    "                \"abstract\": abstract,\n",
    "                \"text\": text_content,\n",
    "                \"journal\": journal_name,\n",
    "                \"date\": issue_date,\n",
    "                \"number\": issue_number\n",
    "            }\n",
    "            # Save JSON file for this article (using title or index for filename)\n",
    "            filename = f\"{article_count}.json\"\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(article_data, f, ensure_ascii=False, indent=4)\n",
    "            articles.append(article_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca44a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03cc599e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Извлечено 76 статей. Сохранено в test_01.01.2021.json\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import json\n",
    "\n",
    "pdf_path = \"01.01.2021.pdf\"\n",
    "output_json = \"test_01.01.2021.json\"\n",
    "\n",
    "doc = fitz.open(pdf_path)\n",
    "\n",
    "cid_to_char = {\n",
    "    'cid:19': 'Ө', 'cid:25': 'ө', 'cid:4': 'Ә', 'cid:8': 'ә', 'cid:5': 'і'\n",
    "}\n",
    "\n",
    "def fix_text(text):\n",
    "    for cid, char in cid_to_char.items():\n",
    "        text = text.replace(f\"({cid})\", char)\n",
    "        text = text.replace(f\"{cid})\", char)\n",
    "        text = text.replace(f\"{cid} \", char)\n",
    "    text = text.replace(\"ə\", \"ә\")\n",
    "    return text\n",
    "\n",
    "def get_abstract(text, num_sentences=10):\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    return \" \".join(sentences[:num_sentences])\n",
    "\n",
    "articles = []\n",
    "journal_name = \"\"\n",
    "issue_date = \"\"\n",
    "issue_number = \"\"\n",
    "article_count = 0\n",
    "\n",
    "for page_index in range(len(doc)):\n",
    "    page = doc.load_page(page_index)\n",
    "    blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "    blocks.sort(key=lambda b: (round(b[\"lines\"][0][\"bbox\"][1]), b[\"lines\"][0][\"bbox\"][0]) if b[\"type\"] == 0 else (float('inf'), float('inf')))\n",
    "\n",
    "    for block in blocks:\n",
    "        if block[\"type\"] != 0:\n",
    "            continue\n",
    "        block_text = \"\".join(span[\"text\"] for line in block[\"lines\"] for span in line[\"spans\"]).strip()\n",
    "        if not block_text:\n",
    "            continue\n",
    "\n",
    "        if page_index == 0:\n",
    "            if not journal_name and re.search(r\"Егемен\\s*Қазақстан|Egemen\\s*Qazaqstan\", block_text):\n",
    "                journal_name = \"Egemen Qazaqstan\"\n",
    "            if not issue_number and re.search(r\"№\\s*\\d+\", block_text):\n",
    "                issue_number = re.search(r\"№\\s*\\d+\", block_text).group(0)\n",
    "            if not issue_date and re.search(r\"\\d{1,2}\\s*[^0-9\\s]{3,}\\s*\\d{4}\", block_text):\n",
    "                issue_date = re.search(r\"\\d{1,2}\\s*[^0-9\\s]{3,}\\s*\\d{4}\", block_text).group(0)\n",
    "\n",
    "        max_size = max((span[\"size\"] for line in block[\"lines\"] for span in line[\"spans\"]), default=0)\n",
    "        is_title_block = max_size > 15\n",
    "\n",
    "        if is_title_block:\n",
    "            article_count += 1\n",
    "            title_lines = []\n",
    "            author_line = \"\"\n",
    "            topic_line = \"\"\n",
    "            body_lines = []\n",
    "\n",
    "            for line in block[\"lines\"]:\n",
    "                if line[\"spans\"]:\n",
    "                    span0 = line[\"spans\"][0]\n",
    "                    text_line = \"\".join(span[\"text\"] for span in line[\"spans\"]).strip()\n",
    "                    if span0[\"size\"] == max_size:\n",
    "                        title_lines.append(text_line)\n",
    "                    elif text_line.isupper():\n",
    "                        topic_line = text_line\n",
    "                    elif \"Italic\" in span0[\"font\"].split('+')[-1] or span0.get(\"flags\", 0) & 2:\n",
    "                        author_line = text_line\n",
    "                    else:\n",
    "                        body_lines.append(text_line)\n",
    "\n",
    "            title = fix_text(\" \".join(title_lines))\n",
    "            topic = fix_text(topic_line)\n",
    "            author = fix_text(author_line)\n",
    "            text_content = \" \".join(body_lines)\n",
    "\n",
    "            continue_index = blocks.index(block) + 1\n",
    "            while continue_index < len(blocks):\n",
    "                next_block = blocks[continue_index]\n",
    "                continue_index += 1\n",
    "                if next_block[\"type\"] != 0:\n",
    "                    continue\n",
    "                next_max = max((span[\"size\"] for line in next_block[\"lines\"] for span in line[\"spans\"]), default=0)\n",
    "                if next_max > 15:\n",
    "                    break\n",
    "                next_text = \"\".join(span[\"text\"] for line in next_block[\"lines\"] for span in line[\"spans\"]).strip()\n",
    "                text_content += \" \" + next_text\n",
    "\n",
    "            text_content = fix_text(text_content)\n",
    "            abstract = get_abstract(text_content)\n",
    "\n",
    "            article_data = {\n",
    "                \"url\": \"\",\n",
    "                \"title\": title,\n",
    "                \"date\": issue_date,\n",
    "                \"author\": author,\n",
    "                \"abstract\": abstract,\n",
    "                \"text\": text_content,\n",
    "                \"journal\": journal_name,\n",
    "                \"category\": topic\n",
    "            }\n",
    "\n",
    "            articles.append(article_data)\n",
    "\n",
    "# Сохраняем все статьи в один JSON-файл\n",
    "with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(articles, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"✅ Извлечено {len(articles)} статей. Сохранено в {output_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63a20532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install sentence-transformers scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd94f1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Результаты для пары: Test_01_01_2021.json vs Train_01_01_2021.json ===\n",
      "title      | Precision: 0.676 | TSS: 0.871 | Holistic: 0.774\n",
      "author     | Precision: 0.735 | TSS: 0.866 | Holistic: 0.801\n",
      "date       | Precision: 1.000 | TSS: 1.000 | Holistic: 1.000\n",
      "abstract   | Precision: 1.000 | TSS: 1.000 | Holistic: 1.000\n",
      "text       | Precision: 0.882 | TSS: 0.927 | Holistic: 0.905\n",
      "journal    | Precision: 1.000 | TSS: 1.000 | Holistic: 1.000\n",
      "category   | Precision: 1.000 | TSS: 1.000 | Holistic: 1.000\n",
      "\n",
      "=== Результаты для пары: Test_20_05_2021.json vs Train_20_05_2021.json ===\n",
      "title      | Precision: 0.800 | TSS: 0.928 | Holistic: 0.864\n",
      "author     | Precision: 1.000 | TSS: 1.000 | Holistic: 1.000\n",
      "date       | Precision: 1.000 | TSS: 1.000 | Holistic: 1.000\n",
      "abstract   | Precision: 0.980 | TSS: 0.959 | Holistic: 0.970\n",
      "text       | Precision: 0.940 | TSS: 0.928 | Holistic: 0.934\n",
      "journal    | Precision: 1.000 | TSS: 1.000 | Holistic: 1.000\n",
      "category   | Precision: 1.000 | TSS: 1.000 | Holistic: 1.000\n",
      "\n",
      "=== Результаты для пары: Test_23_01_2020.json vs Train_23_01_2020.json ===\n",
      "title      | Precision: 0.871 | TSS: 0.933 | Holistic: 0.902\n",
      "author     | Precision: 1.000 | TSS: 1.000 | Holistic: 1.000\n",
      "date       | Precision: 1.000 | TSS: 1.000 | Holistic: 1.000\n",
      "abstract   | Precision: 0.968 | TSS: 0.955 | Holistic: 0.962\n",
      "text       | Precision: 0.871 | TSS: 0.923 | Holistic: 0.897\n",
      "journal    | Precision: 1.000 | TSS: 1.000 | Holistic: 1.000\n",
      "category   | Precision: 1.000 | TSS: 1.000 | Holistic: 1.000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Настройки\n",
    "threshold = 0.85\n",
    "alpha = 0.5\n",
    "tags = [\"title\", \"author\", \"date\", \"abstract\", \"text\", \"journal\", \"category\"]\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Файлы для сравнения (Test, Train)\n",
    "file_pairs = [\n",
    "    (\"Test_01_01_2021.json\", \"Train_01_01_2021.json\"),\n",
    "    (\"Test_20_05_2021.json\", \"Train_20_05_2021.json\"),\n",
    "    (\"Test_23_01_2020.json\", \"Train_23_01_2020.json\"),\n",
    "]\n",
    "\n",
    "# Функция для расчёта метрик\n",
    "def evaluate_pair(test_file, train_file):\n",
    "    with open(test_file, \"r\", encoding=\"utf-8\") as f1, open(train_file, \"r\", encoding=\"utf-8\") as f2:\n",
    "        ref_data = json.load(f1)\n",
    "        test_data = json.load(f2)\n",
    "\n",
    "    assert len(ref_data) == len(test_data), f\"Размеры файлов {test_file} и {train_file} не совпадают!\"\n",
    "\n",
    "    tss_totals = {tag: [] for tag in tags}\n",
    "    precision_counts = {tag: 0 for tag in tags}\n",
    "    N = len(ref_data)\n",
    "\n",
    "    for ref, pred in zip(ref_data, test_data):\n",
    "        for tag in tags:\n",
    "            ref_val = ref.get(tag, \"\")\n",
    "            pred_val = pred.get(tag, \"\")\n",
    "            # Получаем эмбеддинги\n",
    "            emb_ref = model.encode(ref_val)\n",
    "            emb_pred = model.encode(pred_val)\n",
    "            sim = cosine_similarity([emb_ref], [emb_pred])[0][0]\n",
    "            tss_totals[tag].append(sim)\n",
    "            if sim >= threshold:\n",
    "                precision_counts[tag] += 1\n",
    "\n",
    "    print(f\"\\n=== Результаты для пары: {test_file} vs {train_file} ===\")\n",
    "    for tag in tags:\n",
    "        tss_avg = np.mean(tss_totals[tag])\n",
    "        precision = precision_counts[tag] / N\n",
    "        holistic = alpha * precision + (1 - alpha) * tss_avg\n",
    "        print(f\"{tag:10s} | Precision: {precision:.3f} | TSS: {tss_avg:.3f} | Holistic: {holistic:.3f}\")\n",
    "\n",
    "# Запускаем для всех трёх пар\n",
    "for test_file, train_file in file_pairs:\n",
    "    evaluate_pair(test_file, train_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36bd19c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
